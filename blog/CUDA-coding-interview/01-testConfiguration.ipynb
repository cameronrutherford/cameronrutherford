{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Testing CUDA Jupyter Features\"\n",
    "format:\n",
    "    html: default\n",
    "    ipynb: default\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's first make sure we have configured things correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  8 05:55:16 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.01              Driver Version: 546.01       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 980M        On  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   50C    P8              10W / 1... |    798MiB /  4096MiB |     10%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-11-08T05:49:46.421340+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.12\n",
      "IPython version      : 8.17.2\n",
      "\n",
      "Compiler    : GCC 11.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.90.1-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory /home/app/blog/CUDA-coding-interview/src already exists\n",
      "Out bin /home/app/blog/CUDA-coding-interview/result.out\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum element is : -130232120\n",
      "The time required : 0.00288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%cu\n",
    "#include <cstdio>\n",
    "#include <iostream>\n",
    " \n",
    "    using namespace std;\n",
    " \n",
    "__global__ void maxi(int* a, int* b, int n)\n",
    "{\n",
    "    int block = 256 * blockIdx.x;\n",
    "    int max = 0;\n",
    " \n",
    "    for (int i = block; i < min(256 + block, n); i++) {\n",
    " \n",
    "        if (max < a[i]) {\n",
    "            max = a[i];\n",
    "        }\n",
    "    }\n",
    "    b[blockIdx.x] = max;\n",
    "}\n",
    " \n",
    "int main()\n",
    "{\n",
    " \n",
    "    int n;\n",
    "    n = 3 >> 2;\n",
    "    int a[n];\n",
    " \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        a[i] = rand() % n;\n",
    "        cout << a[i] << \"\\t\";\n",
    "    }\n",
    " \n",
    "    cudaEvent_t start, end;\n",
    "    int *ad, *bd;\n",
    "    int size = n * sizeof(int);\n",
    "    cudaMalloc(&ad, size);\n",
    "    cudaMemcpy(ad, a, size, cudaMemcpyHostToDevice);\n",
    "    int grids = ceil(n * 1.0f / 256.0f);\n",
    "    cudaMalloc(&bd, grids * sizeof(int));\n",
    " \n",
    "    dim3 grid(grids, 1);\n",
    "    dim3 block(1, 1);\n",
    " \n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&end);\n",
    "    cudaEventRecord(start);\n",
    " \n",
    "    while (n > 1) {\n",
    "        maxi<<<grids, block>>>(ad, bd, n);\n",
    "        n = ceil(n * 1.0f / 256.0f);\n",
    "        cudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice);\n",
    "    }\n",
    " \n",
    "    cudaEventRecord(end);\n",
    "    cudaEventSynchronize(end);\n",
    " \n",
    "    float time = 0;\n",
    "    cudaEventElapsedTime(&time, start, end);\n",
    " \n",
    "    int ans[2];\n",
    "    cudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost);\n",
    " \n",
    "    cout << \"The maximum element is : \" << ans[0] << endl;\n",
    " \n",
    "    cout << \"The time required : \";\n",
    "    cout << time << endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cool for CUDA basics, and works fine, but it is lacking anything substantial in terms of adding other libraries, and installing cooler dependencies. While this makes exploratory programming and learning quite easy for newbies, you want to us CMake / Spack for more complex build systems and dependencies.\n",
    "\n",
    "That all being said, lets see if we can get something working with cuDF and some of the NVIDIA rapids tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about cccl?\n",
    "\n",
    "Let's try it out. Apparently it should just be a part of the CUDA installation, although there is also CMake support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/detail/libcxx/include/atomic:727,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/atomic:18,\n",
      "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/atomic:14,\n",
      "                 from /tmp/tmpcr1suyx3/40500096-ce82-4eef-b40e-5b8680d38f1d.cu:4:\n",
      "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/detail/libcxx/include/support/atomic/atomic_cuda.h:12:4: error: #error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
      "   12 | #  error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
      "      |    ^~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cu\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <cub/block/block_reduce.cuh>\n",
    "#include <cuda/atomic>\n",
    "#include <cstdio>\n",
    "\n",
    "constexpr int block_size = 256;\n",
    "\n",
    "__global__ void reduce(int const* data, int* result, int N) {\n",
    "  using BlockReduce = cub::BlockReduce<int, block_size>;\n",
    "  __shared__ typename BlockReduce::TempStorage temp_storage;\n",
    "\n",
    "  int const index = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  int sum = 0;\n",
    "  if (index < N) {\n",
    "    sum += data[index];\n",
    "  }\n",
    "  sum = BlockReduce(temp_storage).Sum(sum);\n",
    "\n",
    "  if (threadIdx.x == 0) {\n",
    "    cuda::atomic_ref<int, cuda::thread_scope_device> atomic_result(*result);\n",
    "    atomic_result.fetch_add(sum, cuda::memory_order_relaxed);\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\n",
    "  // Allocate and initialize input data\n",
    "  int const N = 1000;\n",
    "  thrust::device_vector<int> data(N);\n",
    "  thrust::fill(data.begin(), data.end(), 1);\n",
    "\n",
    "  // Allocate output data\n",
    "  thrust::device_vector<int> kernel_result(1);\n",
    "\n",
    "  // Compute the sum reduction of `data` using a custom kernel\n",
    "  int const num_blocks = (N + block_size - 1) / block_size;\n",
    "  reduce<<<num_blocks, block_size>>>(thrust::raw_pointer_cast(data.data()),\n",
    "                                     thrust::raw_pointer_cast(kernel_result.data()),\n",
    "                                     N);\n",
    "\n",
    "  auto const err = cudaDeviceSynchronize();\n",
    "  if (err != cudaSuccess) {\n",
    "    std::cout << \"Error: \" << cudaGetErrorString(err) << std::endl;\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "  // Compute the same sum reduction using Thrust\n",
    "  int const thrust_result = thrust::reduce(thrust::device, data.begin(), data.end(), 0);\n",
    "\n",
    "  // Ensure the two solutions are identical\n",
    "  std::printf(\"Custom kernel sum: %d\\n\", kernel_result[0]);\n",
    "  std::printf(\"Thrust reduce sum: %d\\n\", thrust_result);\n",
    "  assert(kernel_result[0] == thrust_result);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After squashing an initial bug (not having `#include <thrust/execution_policy.h>`), I have run into my first blocker. Atomics aren't supported on my poor 970M...\n",
    "\n",
    "It seems like the thrust kernels on their own might be fine? Lets remove the unwanted atomic and try just the cccl example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <cstdio>\n",
    "\n",
    "int main() {\n",
    "\n",
    "  // Allocate and initialize input data\n",
    "  int const N = 1000;\n",
    "  thrust::device_vector<int> data(N);\n",
    "  thrust::fill(data.begin(), data.end(), 1);\n",
    "\n",
    "  // Allocate output data\n",
    "  thrust::device_vector<int> kernel_result(1);\n",
    "\n",
    "  // Compute the same sum reduction using Thrust\n",
    "  int const thrust_result = thrust::reduce(thrust::device, data.begin(), data.end(), 0);\n",
    "\n",
    "  std::printf(\"Thrust reduce sum: %d\\n\", thrust_result);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about RAPIDS?\n",
    "\n",
    "I was planning on following installation guide and installing with pip/poetry, but both methods were timing out... What I need to do is just pick a subset of packages, and not try and install all of `cudf-cu12 dask-cudf-cu12 cuml-cu12 cugraph-cu12 cuspatial-cu12 cuproj-cu12 cuxfilter-cu12 cucim` when I don't know what I really want immediately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA ipykernel",
   "language": "python",
   "name": "py311-cuda12.2-interview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
